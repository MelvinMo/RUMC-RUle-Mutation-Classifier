{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "daWHBvQgHP6N"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "\n",
        "class Discretization:\n",
        "\n",
        "    @staticmethod\n",
        "    def basedOnEntropy(x : np.ndarray):\n",
        "        labelSet = set(x[:,1]);\n",
        "        labelMap = {}\n",
        "        t = 0;\n",
        "        for element in labelSet:\n",
        "            labelMap[element] = t;\n",
        "            t+=1;\n",
        "        for i in range(0, len(x)):\n",
        "            x[i, 1] = labelMap[x[i, 1]];\n",
        "\n",
        "        sortedX = x[x[:, 0].argsort()];\n",
        "        featureValues = sortedX[:, 0];\n",
        "        featureValues = np.unique(featureValues);\n",
        "        if featureValues.shape[0] < 2:\n",
        "            print(\"one feature had unique value -> shape : {0}\".format(featureValues.shape[0]));\n",
        "            return np.zeros(x.shape[0], dtype=int);\n",
        "\n",
        "        condidateCutPoints = np.zeros(len(featureValues)-1);\n",
        "\n",
        "        for i in range(0, len(featureValues)-1):\n",
        "            condidateCutPoints[i] = (featureValues[i] + featureValues[i+1])/2;\n",
        "\n",
        "        selectedCutPointIndex = -1;\n",
        "        minEntropy = sys.float_info.max;\n",
        "        for i in range(0, len(condidateCutPoints)):\n",
        "            ent = Discretization.__entropy(sortedX, condidateCutPoints[i], len(labelSet));\n",
        "            if ent < minEntropy :\n",
        "                minEntropy = ent;\n",
        "                selectedCutPointIndex = i;\n",
        "        #print(\"cutPoint : {0}\".format(condidateCutPoints[selectedCutPointIndex]));\n",
        "        return Discretization.__generateNewData(x[:,0], condidateCutPoints[selectedCutPointIndex]);\n",
        "\n",
        "    @staticmethod\n",
        "    def __generateNewData(data : np.ndarray, cutpoint):\n",
        "        newData = np.zeros(len(data), dtype=int);\n",
        "        for i in range(0, len(data)):\n",
        "            if(data[i] <= cutpoint):\n",
        "                newData[i] = 0;\n",
        "            else:\n",
        "                newData[i] = 1;\n",
        "        return newData;\n",
        "\n",
        "    @staticmethod\n",
        "    def __entropy(data : np.ndarray, cutpoint, numOfClasses):\n",
        "        entropy = 0;\n",
        "        leftBucketInstanceNumber = 0;\n",
        "        rightBucketInstanceNumber = 0;\n",
        "        allInstanceNumber = len(data);\n",
        "        leftBucket = np.zeros(numOfClasses);\n",
        "        rightBucket = np.zeros(numOfClasses);\n",
        "        for i in range(0, len(data)):\n",
        "            if data[i, 0] <= cutpoint :\n",
        "                leftBucket[data[i, 1]] += 1;\n",
        "                leftBucketInstanceNumber += 1;\n",
        "            else:\n",
        "                rightBucket[data[i, 1]] += 1;\n",
        "                rightBucketInstanceNumber += 1;\n",
        "\n",
        "\n",
        "        leftBucketClassProbability = leftBucket / leftBucketInstanceNumber;\n",
        "        rightBucketClassProbability = rightBucket / rightBucketInstanceNumber;\n",
        "        leftBucketEntropy = np.sum(leftBucketClassProbability * np.log2(leftBucketClassProbability, out=np.zeros_like(leftBucketClassProbability), where=(leftBucketClassProbability!=0)));\n",
        "        rightBucketEntropy = np.sum(rightBucketClassProbability * np.log2(rightBucketClassProbability, out=np.zeros_like(rightBucketClassProbability), where=(rightBucketClassProbability!=0)));\n",
        "\n",
        "        entropy = (leftBucketInstanceNumber / allInstanceNumber) * leftBucketEntropy + (rightBucketInstanceNumber / allInstanceNumber) * rightBucketEntropy;\n",
        "        entropy = -1 * entropy;\n",
        "        return entropy;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UOoHDBFfHbIH"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple, Union\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from typing import Union\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class RUMCPreprocesser:\n",
        "    def __init__(self, framework=\"numpy\"):\n",
        "        assert framework in [\n",
        "            \"numpy\",\n",
        "            \"torch\",\n",
        "        ], \"framework must be either 'numpy' or 'torch'\"\n",
        "        self.framework = framework\n",
        "\n",
        "    def fit_transform(\n",
        "        self, data: Union[pd.DataFrame, np.ndarray], dataTypes\n",
        "    ):\n",
        "        \"\"\"Preprocesses the dataset by replacing nominal values with dummy variables.\n",
        "        Converts to torch bool tensors and returns the dataset. All numerical values are discretized\n",
        "        into equal-sized bins using a quantile-based method (pd.qcut).\n",
        "        Args:\n",
        "            X (pandas.DataFrame or np.ndarray): features vector\n",
        "            y (pandas.DataFrame or np.ndarray): targets vector\n",
        "        Returns:\n",
        "            X (torch.Tensor or np.ndarray): features vector\n",
        "            y (torch.Tensor or np.ndarray): targets vector\n",
        "        \"\"\"\n",
        "\n",
        "        classIndex = data.shape[1]-1;\n",
        "        X = data[:, 0:data.shape[1]-1];\n",
        "        y = data[:, data.shape[1]-1];\n",
        "        #numerics = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "        numAttrIndex = [i for i, t in enumerate(dataTypes) if t == 'numeric' or t == 'integer' or t == 'real'];\n",
        "\n",
        "        if numAttrIndex:\n",
        "            #X[numerics] = pd.qcut(X[numerics], bins=self.bins)\n",
        "            for num in numAttrIndex:\n",
        "                newData = Discretization.basedOnEntropy(data[:,[num,classIndex]]);\n",
        "                X[:, num] = newData;\n",
        "\n",
        "        X = pd.get_dummies(pd.DataFrame(X)).to_numpy()\n",
        "        y = pd.get_dummies(y).to_numpy()\n",
        "        if self.framework == \"torch\":\n",
        "            import torch\n",
        "\n",
        "            X, y = torch.tensor(X, dtype=torch.bool), torch.tensor(y, dtype=torch.bool)\n",
        "        return X, y\n",
        "\n",
        "\n",
        "from numpy import (\n",
        "    logical_and as AND,\n",
        "    logical_not as NOT,\n",
        "    logical_or as OR,\n",
        "    logical_xor as XOR,\n",
        ")\n",
        "\n",
        "\n",
        "def XNOR(input: np.ndarray, other: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Computes the XNOR gate. (semantically the same as `input == other`)\n",
        "    Args:\n",
        "        input (np.ndarray): Input tensor\n",
        "        other (np.ndarray): Other input tensor\n",
        "    Returns:\n",
        "        np.ndarray: Output tensor\n",
        "    \"\"\"\n",
        "    return NOT(XOR(input, other)).astype(bool)\n",
        "\n",
        "\n",
        "class RUMC:\n",
        "    def __init__(\n",
        "        self,\n",
        "        alpha=0.99,\n",
        "        fitness_fn=\"weighted_average\",\n",
        "        gamma=0.6,\n",
        "        suppress_warnings=False,\n",
        "        benchmark=False,\n",
        "    ):\n",
        "        \"\"\"Initialize the RUMC class.\n",
        "        Args:\n",
        "            alpha (float, optional): Value of alpha according to the RUMC paper. Defaults to 0.99.\n",
        "            fitness_function (str in [\"weighted_average\", \"f-beta\"], optional): Choice of fitness function to use. Defaults to \"weighted_average\".\n",
        "            gamma (float, optional): Weight given to coverage score during closest_matching if rules cannot perfectly describe a given input.\n",
        "                                      values > 0.5 recommended. Defaults to 0.6.\n",
        "            suppress_warnings (bool, optional): Whether to suppress any warnings raised during prediction. Defaults to False.\n",
        "            benchmark (bool, optional): Whether to time the `fit` method for benchmark purposes. Defaults to False.\n",
        "        \"\"\"\n",
        "        self._alpha, self._beta = alpha, 1 - alpha\n",
        "        self._gamma = gamma\n",
        "\n",
        "        assert fitness_fn in [\n",
        "            \"weighted_average\",\n",
        "            \"f-beta\",\n",
        "        ], \"fitness_function must be either 'weighted_average' or 'f-beta'\"\n",
        "        self._fitness_fn = (\n",
        "            self._fitness_weighted_avg\n",
        "            if fitness_fn == \"weighted_average\"\n",
        "            else self._fitness_f_beta\n",
        "        )\n",
        "        self._fitness_fn_name = (\n",
        "            \"Weighted Average\" if fitness_fn == \"weighted_average\" else \"F-Beta\"\n",
        "        )\n",
        "\n",
        "        self._suppress_warnings = suppress_warnings\n",
        "\n",
        "        self._benchmark = benchmark\n",
        "\n",
        "        self._has_fit = False\n",
        "\n",
        "        self.cardinality =[]\n",
        "\n",
        "\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
        "        \"\"\"Fits the RUMC algorithm on top of input data X and targets y.\n",
        "        The code is written in a way that it is similar to the pseudo-code provided in the RUMC paper.\n",
        "        Args:\n",
        "            X (np.ndarray): features vector\n",
        "            y (np.ndarray): targets vector\n",
        "        \"\"\"\n",
        "        if self._benchmark:\n",
        "            from time import perf_counter\n",
        "\n",
        "            tic = perf_counter()\n",
        "\n",
        "        self._X, self._y = X, y\n",
        "        self._cardinality, self._rule_len = self._X.shape\n",
        "        self._classes = np.unique(self._y, axis=0)\n",
        "        self._class_indices = {\n",
        "            self._label_to_int(cls): np.where(np.min(XNOR(self._y, cls), axis=-1))[0]\n",
        "            for cls in self._classes\n",
        "        }\n",
        "\n",
        "        self._create_init_rules()\n",
        "\n",
        "        self.RUM(self._extants_if, self._extants_then)\n",
        "\n",
        "        self._generalize_extants()\n",
        "\n",
        "        for cls in self._class_indices.keys():\n",
        "            indices = self._class_indices[cls]\n",
        "            i, j = np.triu_indices(len(indices), k=1)\n",
        "            i, j = indices[i], indices[j]\n",
        "            for i_idx, j_idx in zip(i, j):\n",
        "                self._process_rules(i_idx, j_idx)\n",
        "\n",
        "        independent_indices = ~(self._extants_covered)\n",
        "        self._extants_if, self._extants_then, self._fitnesses = (\n",
        "            self._extants_if[independent_indices],\n",
        "            self._extants_then[independent_indices],\n",
        "            self._fitnesses[independent_indices] ,\n",
        "        )\n",
        "\n",
        "        self._generalize_extants()\n",
        "\n",
        "        args = np.argsort(self._fitnesses)[::-1]\n",
        "        self._final_rules_if, self._final_rules_then, self._fitnesses = (\n",
        "            self._extants_if[args],\n",
        "            self._extants_then[args],\n",
        "            self._fitnesses[args],\n",
        "        )\n",
        "\n",
        "        self._has_fit = True\n",
        "\n",
        "        if self._benchmark:\n",
        "            self._bench_time = perf_counter() - tic\n",
        "\n",
        "\n",
        "    def RUM(self,base_array , base_result):\n",
        "\n",
        "        for i in range(len(base_array)):\n",
        "            temp_result = base_array[i]\n",
        "            temp_fitness = self._fitnesses[i]\n",
        "            false_indices = [idx for idx, value in enumerate(base_array[i]) if not value]\n",
        "            if not false_indices:\n",
        "                continue\n",
        "\n",
        "            for j in range(0,len(false_indices)):\n",
        "                new_array = base_array[i].copy()\n",
        "                new_array[false_indices[j]] = True\n",
        "                fitness = self._fitness_fn(\n",
        "                        new_array, base_result[i]\n",
        "                    )\n",
        "                if(fitness > temp_fitness):\n",
        "                    temp_fitness = fitness\n",
        "                    temp_result = new_array\n",
        "            self._extants_if[i] = temp_result\n",
        "            self._fitnesses[i] = temp_fitness\n",
        "\n",
        "\n",
        "    def predict(self, X: np.ndarray, convert_dummies=True) -> np.ndarray:\n",
        "        \"\"\"Given input X, predict label using RUMC\n",
        "        Args:\n",
        "            X (np.ndarray): input features vector\n",
        "            convert_dummies (bool): whether to convert the output to a one-dimensional array\n",
        "        Returns:\n",
        "            np.ndarray: label as predicted by RUMC\n",
        "        \"\"\"\n",
        "        assert self._has_fit, \"RUMC has not been fit yet.\"\n",
        "        labels = np.zeros((len(X), self._final_rules_then.shape[1]), dtype=bool)\n",
        "        found = np.zeros(len(X), dtype=bool)\n",
        "        for i in range(len(self._final_rules_if)):\n",
        "            covered = self._covered(X, self._final_rules_if[i])\n",
        "            labels[AND(covered, NOT(found))] = self._final_rules_then[i]\n",
        "            found[covered] = True\n",
        "            if found.sum() == len(X):  # -> every instance was matched to a rule\n",
        "                break\n",
        "\n",
        "        all_found = found.sum() == len(X)\n",
        "        if not all_found:\n",
        "            print(\n",
        "                f\"Warning: RUMC was unable to find a perfect match for {len(X) - found.sum()} instances out of {len(X)}.\"\n",
        "            )\n",
        "            print(\n",
        "                \"Labels for these instances will be determined by a closest match algorithm.\"\n",
        "            )\n",
        "            leftover_indices = np.where(~found)[0]\n",
        "            for idx in leftover_indices:\n",
        "                labels[idx] = self._closest_match(X[idx])\n",
        "\n",
        "        if convert_dummies:\n",
        "            labels = np.argmax(labels, axis=-1)\n",
        "\n",
        "        return labels\n",
        "\n",
        "    def display_rules(self):\n",
        "        \"\"\"Print out the final rules\"\"\"\n",
        "        assert self._has_fit, \"RUMC has not been fit yet.\"\n",
        "        print(\"Algorithm Parameters:\")\n",
        "        print(f\"\\t- Fitness Function: {self._fitness_fn_name}\")\n",
        "        print(f\"\\t- Alpha: {self._alpha}\")\n",
        "        print(f\"\\t- Gamma: {self._gamma}\")\n",
        "        if self._benchmark:\n",
        "            print(f\"\\t- Compute Device: CPU\")\n",
        "            print(f\"\\t- Time to fit: {self._bench_time}s\")\n",
        "        print(\n",
        "            f\"\\nFinal Rules ({len(self._final_rules_if)} total): (if --> then (label) | fitness)\"\n",
        "        )\n",
        "        for i in range(len(self._final_rules_if)):\n",
        "            print(\n",
        "                f\"\\t{np.array2string(self._final_rules_if[i].astype(int), separator='')} -->\"\n",
        "                f\" {np.array2string(self._final_rules_then[i].astype(int), separator='')}\"\n",
        "                f\" ({np.argmax(self._final_rules_then[i].astype(int))})\"\n",
        "                f\" | {self._fitnesses[i]}\"\n",
        "            )\n",
        "\n",
        "    def _closest_match(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Find the closest matching rule to `X`\n",
        "        Args:\n",
        "            X (np.ndarray): input `X`\n",
        "        Returns:\n",
        "            np.ndarray: matched rule\n",
        "        \"\"\"\n",
        "        int_X = X.astype(int)  # <- cast boolean array to integer array\n",
        "        overlap = OR(NOT(int_X), AND(self._final_rules_if, int_X)).sum(axis=-1)\n",
        "        overlap = overlap / self._rule_len  # -> normalize by rule length\n",
        "        scores = np.multiply(self._gamma * overlap, (1 - self._gamma) * self._fitnesses)\n",
        "        argmax = np.argmax(scores)\n",
        "        return self._final_rules_then[argmax]\n",
        "\n",
        "    def score(self, X_test: np.ndarray, y_test: np.ndarray) -> float:\n",
        "        \"\"\"Returns accuracy on the provided test data\n",
        "        Args:\n",
        "            X_test (np.ndarray): test features vector\n",
        "            y_test (np.ndarray): test targets vector\n",
        "        Returns:\n",
        "            float: accuracy score\n",
        "        \"\"\"\n",
        "        assert self._has_fit, \"RUMC has not been fit yet.\"\n",
        "        X_test, y_test = X_test.astype(np.float32), y_test.astype(np.float32)\n",
        "        if y_test.ndim != 1 and y_test.shape[1] != 1:\n",
        "            y_test = np.argmax(y_test, axis=-1)\n",
        "        y_pred = self.predict(X_test)\n",
        "        return accuracy_score(y_test, y_pred)\n",
        "\n",
        "    def _fitness_weighted_avg(\n",
        "        self, rule_if: np.ndarray, rule_then: np.ndarray\n",
        "    ) -> float:\n",
        "        \"\"\"Returns fitness for a given rule according to the RUMC paper\n",
        "        Args:\n",
        "            rule_if (np.ndarray): if part of a rule (x)\n",
        "            rule_then (np.ndarray): then part of a rule (y)\n",
        "        Returns:\n",
        "            float: fitness score for the rule as defined in the RUMC paper\n",
        "        \"\"\"\n",
        "\n",
        "        n_covered, n_correct = self._confusion(rule_if, rule_then)\n",
        "        if n_covered != 0 :\n",
        "            accuracy = n_correct / n_covered\n",
        "            coverage = n_covered / self._cardinality\n",
        "        else :\n",
        "            accuracy = 0\n",
        "            coverage = 0\n",
        "\n",
        "        return self._alpha * accuracy + self._beta * coverage\n",
        "\n",
        "    def _fitness_f_beta(self, rule_if: np.ndarray, rule_then: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Returns f-beta-score fitness for a given rule\n",
        "        Args:\n",
        "            rule_if (np.ndarray): if part of a rule (x)\n",
        "            rule_then (np.ndarray): then part of a rule (y)\n",
        "        Returns:\n",
        "            np.ndarray: f-beta-score fitness for the rule\n",
        "        \"\"\"\n",
        "        # `beta` in f-beta-score is chosen such that recall is considered `beta` times as important as precision\n",
        "        # https://en.wikipedia.org/wiki/F-score\n",
        "        beta = self._beta / self._alpha\n",
        "        n_covered, n_correct = self._confusion(rule_if, rule_then)\n",
        "        accuracy = n_correct / n_covered\n",
        "        coverage = n_covered / self._cardinality\n",
        "        return (\n",
        "            (1 + beta**2) * (accuracy * coverage) / (beta**2 * accuracy + coverage)\n",
        "        )\n",
        "\n",
        "    def _covered(self, X: np.ndarray, rule_if: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Returns indices of instances in `X` that are covered by `rule_if`.\n",
        "        Note that rule covers instance if EITHER of the following holds in a bitwise manner:\n",
        "        1. instance[i] == 0\n",
        "        2. instance[1] == 1 AND rule[i] == 1\n",
        "        Args:\n",
        "            X (np.ndarray): instances\n",
        "            rule_if (np.ndarray): if part of rule (x)\n",
        "        Returns:\n",
        "            np.ndarray: An array containing indices of instances in `X_same_class` that are covered by `rule_if`\n",
        "        \"\"\"\n",
        "        covered = OR(NOT(X[:self._cardinality,:]), AND(rule_if, X[:self._cardinality,:])).min(axis=-1)\n",
        "        return covered\n",
        "\n",
        "    def _confusion(\n",
        "        self, rule_if: np.ndarray, rule_then: np.ndarray\n",
        "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"Returns n_correct and n_covered for instances classified by a rule.\n",
        "        Args:\n",
        "            rule_if (np.ndarray): if part of rule (x)\n",
        "            rule_then (np.ndarray): then part of rule (y)\n",
        "        Returns:\n",
        "            Tuple[np.ndarray, np.ndarray]: (n_covered, n_correct)\n",
        "        \"\"\"\n",
        "        covered = self._covered(self._X, rule_if)\n",
        "        covered_mask = np.zeros(self._cardinality, dtype=bool)\n",
        "        covered_mask[:self._cardinality] = covered\n",
        "        n_covered = covered.sum()\n",
        "        y_covered = self._y[covered_mask]\n",
        "        n_correct = XNOR(y_covered, rule_then).min(axis=-1).sum()\n",
        "        return n_covered, n_correct\n",
        "\n",
        "    def _create_init_rules(self):\n",
        "        \"\"\"Creates an initial set of rules from the input feature vectors\"\"\"\n",
        "        self._extants_if = self._X.copy()\n",
        "        self._extants_then = self._y.copy()\n",
        "        self._extants_covered = np.zeros(len(self._X), dtype=bool)\n",
        "        self._fitnesses = np.array(\n",
        "            [\n",
        "                self._fitness_fn(rule_if, rule_then)\n",
        "                for rule_if, rule_then in zip(self._X, self._y)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def _composable(self, idx1: int, idx2: int) -> bool:\n",
        "        \"\"\"Returns true if two rules indicated by their indices are composable\n",
        "        Args:\n",
        "            idx1 (int): index of the first rule\n",
        "            idx2 (int): index of the second rule\n",
        "        Returns:\n",
        "            bool: True if labels match and neither of the rules are covered. False otherwise.\n",
        "        \"\"\"\n",
        "        labels_match = XNOR(self._extants_then[idx1], self._extants_then[idx2]).min() # This line is not nessecary since we already check this\n",
        "        return (\n",
        "            labels_match\n",
        "            and not self._extants_covered[idx1].all()\n",
        "            and not self._extants_covered[idx2].all()\n",
        "        )\n",
        "\n",
        "    def _process_rules(self, idx1: int, idx2: int):\n",
        "        \"\"\"Process two rules indicated by their indices\n",
        "        Args:\n",
        "            idx1 (int): index of the first rule\n",
        "            idx2 (int): index of the second rule\n",
        "        \"\"\"\n",
        "        if self._composable(idx1, idx2):\n",
        "            composition = self._compose(self._extants_if[idx1], self._extants_if[idx2])\n",
        "            composition_fitness = self._fitness_fn(\n",
        "                composition, self._extants_then[idx1]\n",
        "            )\n",
        "\n",
        "            if composition_fitness > np.maximum(\n",
        "                self._fitnesses[idx1], self._fitnesses[idx2]\n",
        "            ):\n",
        "                self._update_extants(\n",
        "                    idx1, composition, self._extants_then[idx1], composition_fitness\n",
        "                )\n",
        "\n",
        "    def _compose(self, rule1: np.ndarray, rule2: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Composes rule1 with rule2\n",
        "        Args:\n",
        "            rule1 (np.ndarray): the first rule\n",
        "            rule2 (np.ndarray): the second rule\n",
        "        Returns:\n",
        "            np.ndarray: The composed rule which is simply the bitwise OR of the two rules\n",
        "        \"\"\"\n",
        "        return OR(rule1, rule2)\n",
        "\n",
        "    def _update_extants(\n",
        "        self,\n",
        "        index: int,\n",
        "        new_rule_if: np.ndarray,\n",
        "        new_rule_then: np.ndarray,\n",
        "        new_rule_fitness: np.ndarray,\n",
        "    ):\n",
        "        \"\"\"Remove all rules from current extants that are covered by `new_rule`.\n",
        "        Then append new rule to extants.\n",
        "        Args:\n",
        "            index (int): index of `new_rule`\n",
        "            new_rule_if (np.ndarray): if part of `new_rule` (x)\n",
        "            new_rule_then (np.ndarray): then part of `new_rule` (y)\n",
        "            new_rule_fitness (np.ndarray): fitness of the `new_rule`\n",
        "        \"\"\"\n",
        "        same_class_indices = self._class_indices[self._label_to_int(new_rule_then)]\n",
        "        covered = self._covered(self._extants_if[same_class_indices], new_rule_if)\n",
        "        covered_mask = np.zeros(len(same_class_indices), dtype=bool)\n",
        "        covered_mask[:self._cardinality] = covered\n",
        "        self._extants_covered[same_class_indices[covered_mask]] = True\n",
        "        self._extants_covered[index] = False  # -> except new rule from covered\n",
        "        self._extants_if[index], self._extants_then[index], self._fitnesses[index] = (\n",
        "            new_rule_if,\n",
        "            new_rule_then,\n",
        "            new_rule_fitness,\n",
        "        )\n",
        "\n",
        "    def _label_to_int(self, label: np.ndarray) -> int:\n",
        "        \"\"\"Converts dummy label to int\n",
        "        Args:\n",
        "            label (np.ndarray): label to convert\n",
        "        Returns:\n",
        "            int: converted label\n",
        "        \"\"\"\n",
        "        return int(np.argmax(label))\n",
        "\n",
        "    def _generalize_extants(self):\n",
        "        \"\"\"Generalize the extants by flipping every 0 to a 1 and checking if the fitness improves.\"\"\"\n",
        "        new_extants_if = np.zeros_like(self._extants_if, dtype=bool)\n",
        "        for i in range(len(self._extants_if)):\n",
        "            for j in range(len(self._extants_if[i])):\n",
        "                if not self._extants_if[i][j]:\n",
        "                    self._extants_if[i][j] = True\n",
        "                    fitness = self._fitness_fn(\n",
        "                        self._extants_if[i], self._extants_then[i]\n",
        "                    )\n",
        "                    if fitness > self._fitnesses[i]:\n",
        "                        self._fitnesses[i] = fitness\n",
        "                    else:\n",
        "                        self._extants_if[i][j] = False\n",
        "            new_extants_if[i] = self._extants_if[i]\n",
        "        self._extants_if = new_extants_if\n",
        "\n",
        "\n",
        "    def getNumOfRules(self):\n",
        "        return len(self._final_rules_if);\n",
        "\n",
        "    def reduceRules(self):\n",
        "        assert self._has_fit, \"RUMC has not been fit yet.\"\n",
        "        i = 0;\n",
        "        j = 0;\n",
        "        while(i < len(self._final_rules_if)-1):\n",
        "            j = i+1;\n",
        "            coveredIndex = [];\n",
        "            while(j < len(self._final_rules_if)):\n",
        "                covered = True;\n",
        "                for t in range(len(self._final_rules_if[j])):\n",
        "                    if self._final_rules_if[j][t] and not self._final_rules_if[i][t]:\n",
        "                        covered = False;\n",
        "                        break;\n",
        "                if covered :\n",
        "                    coveredIndex.append(j);\n",
        "                j += 1;\n",
        "            self._final_rules_if = np.delete(self._final_rules_if, coveredIndex, axis = 0);\n",
        "            self._final_rules_then = np.delete(self._final_rules_then, coveredIndex, axis = 0);\n",
        "            self._fitnesses = np.delete(self._fitnesses, coveredIndex, axis = 0);\n",
        "            i += 1;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoKLoPMQHhs1"
      },
      "outputs": [],
      "source": [
        "from scipy.io import arff\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.io import arff\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\"\"\"\n",
        "# Convert the ARFF data to a structured NumPy array\n",
        "data_array = np.array(data.tolist(), dtype=data.dtype)\n",
        "print(\"Data Array Shape:\", data_array.shape)\n",
        "\n",
        "# Separate features (X) and labels (y)\n",
        "X = data_array[:,:-1] # Features\n",
        "y = data_array[:,-1]  # Labels\n",
        "\n",
        "# Modify the dataset path below to test other ARFF files.\n",
        "\"\"\"\n",
        "\n",
        "data, meta = arff.loadarff(\"/content/data/1-chscase_vine1.arff\")\n",
        "dataTypes = meta.types();\n",
        "dataSet = pd.DataFrame(data).values;\n",
        "preprocessor = RUMCPreprocesser();\n",
        "X, y = preprocessor.fit_transform(dataSet, dataTypes);\n",
        "accuracy = 0 ;\n",
        "numOfRules = 0;\n",
        "n_splits1 = 10;\n",
        "kf = KFold(n_splits=n_splits1 , random_state=1 , shuffle=True)\n",
        "i=0\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train , X_test , Y_train , Y_test = [] , [] , [] , []\n",
        "    X_train , X_test = X[train_index] , X[test_index]\n",
        "    Y_train , Y_test = y[train_index] , y[test_index]\n",
        "    rumc = None\n",
        "    rumc = RUMC(alpha = 0.99, gamma = 0.6, suppress_warnings = True);\n",
        "    rumc.fit(X_train, Y_train);\n",
        "    rumc.reduceRules();\n",
        "    score = rumc.score(X_test, Y_test);\n",
        "    rules = rumc.getNumOfRules();\n",
        "    numOfRules += rules;\n",
        "    accuracy += score;\n",
        "    print(\"Results for the fold {0}:\".format(i));\n",
        "    print(\"Accuracy: {0}\".format(score));\n",
        "    print(\"Number of rules generated: {0}\".format(rules));\n",
        "    print(\"\\n\");\n",
        "    i=i+1\n",
        "\n",
        "\n",
        "print(\"#####################\\n\");\n",
        "print(\"\\tFinal results for {0} folds:\".format(n_splits1))\n",
        "print(\"\\tTotal number of rules generated: {0}\".format(numOfRules/10));\n",
        "print(\"\\tFinal accuracy in percentage: {:.2f}\".format((accuracy/10)*100));\n",
        "print(\"\\n#####################\");"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}